# LRU Cache Optimization Guide

## Current Implementation Analysis

Your current LRU cache is simple and effective, but there's room for significant optimization! Here are **10 proven strategies** to improve performance, hit rates, and reduce costs.

---

## ðŸš€ Optimization Strategies

### 1. **Semantic Similarity Caching** â­ HIGH IMPACT

**Problem:** Similar questions generate different cache keys
```
"Who won Physics?" â‰  "Which scholars won prizes in Physics?"
Both need full pipeline even though answers are similar!
```

**Solution:** Use embeddings to find similar cached questions

**Implementation:**

```python
# cache_method/semantic_cache.py
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import Optional, Tuple

class SemanticLRUCache(LRUDataManager):
    def __init__(self, cache_size=256, similarity_threshold=0.85):
        super().__init__(cache_size=cache_size)
        self.model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, lightweight
        self.similarity_threshold = similarity_threshold
        self.question_embeddings = {}  # Store embeddings
    
    def _get_embedding(self, text: str) -> np.ndarray:
        """Generate embedding for text."""
        return self.model.encode(text, convert_to_tensor=False)
    
    def find_similar_cached(self, question: str) -> Optional[Tuple[str, float]]:
        """Find most similar cached question above threshold."""
        if not self.question_embeddings:
            return None
        
        query_emb = self._get_embedding(question)
        
        best_key = None
        best_similarity = 0.0
        
        for key, cached_emb in self.question_embeddings.items():
            # Cosine similarity
            similarity = np.dot(query_emb, cached_emb) / (
                np.linalg.norm(query_emb) * np.linalg.norm(cached_emb)
            )
            
            if similarity > best_similarity and similarity >= self.similarity_threshold:
                best_similarity = similarity
                best_key = key
        
        return (best_key, best_similarity) if best_key else None
    
    def get_data_semantic(self, question: str, cache_key: str):
        """Try exact match first, then semantic match."""
        # Try exact match
        result = self.get_data(cache_key)
        if result:
            return result, "exact"
        
        # Try semantic match
        similar = self.find_similar_cached(question)
        if similar:
            similar_key, similarity = similar
            result = self.cache.get(similar_key)
            if result:
                return result, f"similar ({similarity:.2f})"
        
        return None, "miss"
    
    def set_data_semantic(self, question: str, cache_key: str, value):
        """Store data with embedding."""
        self.set_data(cache_key, value)
        self.question_embeddings[cache_key] = self._get_embedding(question)
```

**Benefits:**
- ðŸ“ˆ **+20-40% hit rate** improvement
- ðŸ’° Saves on similar query variations
- ðŸŽ¯ Better user experience

---

### 2. **Query Normalization** â­ HIGH IMPACT

**Problem:** Trivial differences create different keys
```
"Who won Physics?"
"Who won physics?"  â† Different case
"Who won  Physics?" â† Extra space
```

**Solution:** Normalize queries before caching

```python
# cache_method/normalizer.py
import re

class QueryNormalizer:
    @staticmethod
    def normalize(question: str) -> str:
        """Normalize question for consistent caching."""
        # Lowercase
        question = question.lower().strip()
        
        # Remove extra whitespace
        question = re.sub(r'\s+', ' ', question)
        
        # Remove punctuation at end
        question = question.rstrip('?.!')
        
        # Expand contractions
        contractions = {
            "who's": "who is",
            "what's": "what is",
            "won't": "will not",
            "can't": "cannot",
        }
        for contraction, expansion in contractions.items():
            question = question.replace(contraction, expansion)
        
        return question

# Usage in graph_rag_workflow.py:
normalized_question = QueryNormalizer.normalize(question)
cache_key = f"{normalized_question}|{schema_str}"
```

**Benefits:**
- ðŸ“ˆ **+10-15% hit rate** improvement
- ðŸŽ¯ Catches trivial variations
- ðŸš€ Easy to implement

---

### 3. **Cache Warming** â­ MEDIUM IMPACT

**Problem:** Cold start - first queries are always slow

**Solution:** Pre-populate cache with common queries

```python
# cache_warming.py
def warm_cache(
    cache_manager,
    db_manager,
    common_questions: list[str]
):
    """Pre-populate cache with common questions."""
    print("ðŸ”¥ Warming cache...")
    
    for question in common_questions:
        # Run full pipeline to populate cache
        result = process_question(
            question=question,
            conn=db_manager.conn,
            # ... other params
        )
    
    print(f"âœ… Cache warmed with {len(common_questions)} queries")
    print(f"ðŸ“¦ Cache size: {len(cache_manager.cache)}/{cache_manager.cache.maxsize}")

# Common queries for Nobel Prize dataset
COMMON_QUERIES = [
    "Who won the Nobel Prize in Physics?",
    "List all Nobel laureates in Chemistry",
    "How many prizes were awarded after 2000?",
    "Which scholars won multiple Nobel Prizes?",
    "List all female Nobel laureates",
    "Who was affiliated with MIT?",
    "Which country has the most Nobel Prize winners?",
]

# In main():
warm_cache(lru_cache_manager, db_manager, COMMON_QUERIES)
```

**Benefits:**
- âš¡ Immediate performance for common queries
- ðŸ“ˆ Better user experience from start
- ðŸ’° Reduces cost for predictable workloads

---

### 4. **TTL (Time-To-Live)** â­ MEDIUM IMPACT

**Problem:** Stale data stays in cache forever

**Solution:** Add expiration times

```python
# cache_method/ttl_cache.py
import time
from typing import Any, Optional

class TTLLRUCache(LRUDataManager):
    def __init__(self, cache_size=256, default_ttl=3600):
        super().__init__(cache_size=cache_size)
        self.ttl_store = {}  # key -> expiration_time
        self.default_ttl = default_ttl  # seconds
    
    def set_data(self, key, value, ttl: Optional[int] = None):
        """Store data with TTL."""
        super().set_data(key, value)
        expiration = time.time() + (ttl or self.default_ttl)
        self.ttl_store[key] = expiration
    
    def get_data(self, key):
        """Get data if not expired."""
        # Check expiration
        if key in self.ttl_store:
            if time.time() > self.ttl_store[key]:
                # Expired - remove from cache
                self.cache.pop(key, None)
                self.ttl_store.pop(key, None)
                return None
        
        return super().get_data(key)
    
    def cleanup_expired(self):
        """Remove all expired entries."""
        current_time = time.time()
        expired_keys = [
            key for key, exp_time in self.ttl_store.items()
            if current_time > exp_time
        ]
        
        for key in expired_keys:
            self.cache.pop(key, None)
            self.ttl_store.pop(key, None)
        
        return len(expired_keys)

# Usage:
cache = TTLLRUCache(cache_size=256, default_ttl=3600)  # 1 hour TTL
cache.set_data(key, value, ttl=7200)  # Custom 2 hour TTL
```

**Benefits:**
- ðŸ”„ Fresh data automatically
- ðŸ’¾ Better memory usage
- ðŸŽ¯ Configurable per query type

---

### 5. **Adaptive Cache Sizing** â­ MEDIUM IMPACT

**Problem:** Fixed cache size may beuv too small or too large

**Solution:** Dynamically adjust based on hit rate

```python
# cache_method/adaptive_cache.py
class AdaptiveLRUCache(LRUDataManager):
    def __init__(self, initial_size=256, min_size=64, max_size=1024):
        super().__init__(cache_size=initial_size)
        self.min_size = min_size
        self.max_size = max_size
        self.hit_rate_window = []
        self.window_size = 100
        self.adjustment_threshold = 0.1
    
    def record_access(self, was_hit: bool):
        """Record cache access for adaptation."""
        self.hit_rate_window.append(1 if was_hit else 0)
        
        # Keep window size limited
        if len(self.hit_rate_window) > self.window_size:
            self.hit_rate_window.pop(0)
        
        # Adjust if window is full
        if len(self.hit_rate_window) == self.window_size:
            self._maybe_adjust_size()
    
    def _maybe_adjust_size(self):
        """Adjust cache size based on hit rate."""
        hit_rate = sum(self.hit_rate_window) / len(self.hit_rate_window)
        current_size = self.cache.maxsize
        
        # Low hit rate AND not at max -> increase
        if hit_rate < 0.4 and current_size < self.max_size:
            new_size = min(int(current_size * 1.5), self.max_size)
            self._resize_cache(new_size)
            print(f"ðŸ“ˆ Cache resized: {current_size} â†’ {new_size} (hit rate: {hit_rate:.1%})")
        
        # High hit rate AND cache underutilized -> decrease
        elif hit_rate > 0.8 and len(self.cache) < current_size * 0.7 and current_size > self.min_size:
            new_size = max(int(current_size * 0.8), self.min_size)
            self._resize_cache(new_size)
            print(f"ðŸ“‰ Cache resized: {current_size} â†’ {new_size} (hit rate: {hit_rate:.1%})")
    
    def _resize_cache(self, new_size: int):
        """Resize cache preserving most recent entries."""
        old_cache = self.cache
        self.cache = LRUCache(maxsize=new_size)
        
        # Copy most recent entries
        for key in list(old_cache.keys()):
            if len(self.cache) >= new_size:
                break
            self.cache[key] = old_cache[key]
```

**Benefits:**
- ðŸŽ¯ Optimal cache size automatically
- ðŸ’¾ Better memory efficiency
- ðŸ“ˆ Adapts to workload changes

---

### 6. **Compression for Large Values** â­ LOW IMPACT (but useful)

**Problem:** Large context data wastes memory

**Solution:** Compress cached values

```python
# cache_method/compressed_cache.py
import zlib
import pickle

class CompressedLRUCache(LRUDataManager):
    def __init__(self, cache_size=256, compression_threshold=1024):
        super().__init__(cache_size=cache_size)
        self.compression_threshold = compression_threshold  # bytes
    
    def set_data(self, key, value):
        """Store data with compression if large."""
        serialized = pickle.dumps(value)
        
        if len(serialized) > self.compression_threshold:
            compressed = zlib.compress(serialized, level=6)
            super().set_data(key, {
                "compressed": True,
                "data": compressed
            })
        else:
            super().set_data(key, {
                "compressed": False,
                "data": serialized
            })
    
    def get_data(self, key):
        """Retrieve and decompress if needed."""
        result = super().get_data(key)
        if not result:
            return None
        
        if result["compressed"]:
            decompressed = zlib.decompress(result["data"])
            return pickle.loads(decompressed)
        else:
            return pickle.loads(result["data"])
```

**Benefits:**
- ðŸ’¾ 2-5x memory savings for large contexts
- ðŸš€ Minimal CPU overhead
- ðŸ“¦ More entries fit in cache

---

### 7. **Multi-Level Caching** â­ HIGH IMPACT (advanced)

**Problem:** Single cache layer may not be optimal

**Solution:** L1 (fast, small) + L2 (slower, large)

```python
# cache_method/tiered_cache.py
class TieredCache:
    def __init__(self, l1_size=64, l2_size=512):
        self.l1 = LRUDataManager(cache_size=l1_size)  # Hot cache
        self.l2 = LRUDataManager(cache_size=l2_size)  # Warm cache
        self.stats = {"l1_hits": 0, "l2_hits": 0, "misses": 0}
    
    def get_data(self, key):
        """Check L1, then L2."""
        # Try L1 (hot cache)
        result = self.l1.get_data(key)
        if result:
            self.stats["l1_hits"] += 1
            return result
        
        # Try L2 (warm cache)
        result = self.l2.get_data(key)
        if result:
            self.stats["l2_hits"] += 1
            # Promote to L1
            self.l1.set_data(key, result)
            return result
        
        self.stats["misses"] += 1
        return None
    
    def set_data(self, key, value):
        """Store in L1 (will overflow to L2 naturally)."""
        # Always store in L1
        evicted = self.l1.set_data(key, value)
        
        # If L1 evicted something, store in L2
        if evicted:
            self.l2.set_data(evicted[0], evicted[1])
    
    def get_stats(self):
        """Get cache performance stats."""
        total = sum(self.stats.values())
        if total == 0:
            return self.stats
        
        return {
            **self.stats,
            "l1_hit_rate": self.stats["l1_hits"] / total * 100,
            "l2_hit_rate": self.stats["l2_hits"] / total * 100,
            "total_hit_rate": (self.stats["l1_hits"] + self.stats["l2_hits"]) / total * 100
        }
```

**Benefits:**
- âš¡ Ultra-fast for hottest queries
- ðŸ“¦ Large capacity for warm queries
- ðŸŽ¯ Better hit rates overall

---

### 8. **Partial Result Caching** â­ MEDIUM IMPACT

**Problem:** Caching full pipeline is all-or-nothing

**Solution:** Cache intermediate steps separately

```python
# Cache different pipeline stages
class MultiStageCache:
    def __init__(self):
        self.schema_cache = LRUDataManager(64)
        self.query_cache = LRUDataManager(256)
        self.context_cache = LRUDataManager(512)
    
    def get_cached_query(self, question, schema):
        """Try to get cached Cypher query."""
        key = f"query:{question}|{schema}"
        return self.query_cache.get_data(key)
    
    def get_cached_context(self, query):
        """Try to get cached query results."""
        key = f"context:{query}"
        return self.context_cache.get_data(key)
    
    def cache_query(self, question, schema, query):
        """Cache generated query."""
        key = f"query:{question}|{schema}"
        self.query_cache.set_data(key, query)
    
    def cache_context(self, query, context):
        """Cache query execution results."""
        key = f"context:{query}"
        self.context_cache.set_data(key, context)
```

**Benefits:**
- ðŸ”„ Reuse intermediate results
- ðŸ’° Cheaper than full regeneration
- ðŸŽ¯ More flexible caching strategy

---

### 9. **Batch Cache Operations** â­ LOW IMPACT (optimization)

**Problem:** Individual cache operations can be slow

**Solution:** Batch gets/sets

```python
# cache_method/batch_cache.py
class BatchLRUCache(LRUDataManager):
    def get_many(self, keys: list[str]) -> dict:
        """Get multiple keys at once."""
        return {
            key: self.get_data(key)
            for key in keys
            if self.get_data(key) is not None
        }
    
    def set_many(self, items: dict):
        """Set multiple key-value pairs."""
        for key, value in items.items():
            self.set_data(key, value)
    
    def delete_many(self, keys: list[str]):
        """Delete multiple keys."""
        for key in keys:
            self.cache.pop(key, None)
```

**Benefits:**
- âš¡ Faster bulk operations
- ðŸ”„ Useful for cache warming
- ðŸ“¦ Better throughput

---

### 10. **Cache Analytics & Monitoring** â­ HIGH IMPACT

**Problem:** Can't optimize what you don't measure

**Solution:** Comprehensive cache metrics

```python
# cache_method/monitored_cache.py
from collections import Counter
import time

class MonitoredLRUCache(LRUDataManager):
    def __init__(self, cache_size=256):
        super().__init__(cache_size=cache_size)
        self.metrics = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "evictions": 0,
            "total_get_time": 0.0,
            "total_set_time": 0.0,
        }
        self.key_access_count = Counter()
        self.key_last_access = {}
    
    def get_data(self, key):
        """Get with metrics."""
        start = time.time()
        result = super().get_data(key)
        elapsed = time.time() - start
        
        self.metrics["total_get_time"] += elapsed
        
        if result:
            self.metrics["hits"] += 1
            self.key_access_count[key] += 1
            self.key_last_access[key] = time.time()
        else:
            self.metrics["misses"] += 1
        
        return result
    
    def set_data(self, key, value):
        """Set with metrics."""
        start = time.time()
        
        # Check if eviction will happen
        if len(self.cache) >= self.cache.maxsize and key not in self.cache:
            self.metrics["evictions"] += 1
        
        super().set_data(key, value)
        
        elapsed = time.time() - start
        self.metrics["total_set_time"] += elapsed
        self.metrics["sets"] += 1
        self.key_last_access[key] = time.time()
    
    def get_metrics(self) -> dict:
        """Get comprehensive cache metrics."""
        total_ops = self.metrics["hits"] + self.metrics["misses"]
        hit_rate = self.metrics["hits"] / total_ops * 100 if total_ops > 0 else 0
        
        return {
            **self.metrics,
            "hit_rate": hit_rate,
            "cache_size": len(self.cache),
            "cache_capacity": self.cache.maxsize,
            "fill_rate": len(self.cache) / self.cache.maxsize * 100,
            "avg_get_time": self.metrics["total_get_time"] / total_ops if total_ops > 0 else 0,
            "avg_set_time": self.metrics["total_set_time"] / self.metrics["sets"] if self.metrics["sets"] > 0 else 0,
            "most_accessed_keys": self.key_access_count.most_common(10),
        }
    
    def print_report(self):
        """Print comprehensive cache report."""
        metrics = self.get_metrics()
        
        print("\n" + "="*60)
        print("ðŸ“Š CACHE PERFORMANCE REPORT")
        print("="*60)
        print(f"Hit Rate:           {metrics['hit_rate']:.1f}%")
        print(f"Total Hits:         {metrics['hits']}")
        print(f"Total Misses:       {metrics['misses']}")
        print(f"Cache Fill:         {metrics['fill_rate']:.1f}%")
        print(f"Evictions:          {metrics['evictions']}")
        print(f"Avg Get Time:       {metrics['avg_get_time']*1000:.2f}ms")
        print(f"Avg Set Time:       {metrics['avg_set_time']*1000:.2f}ms")
        print(f"\nTop 5 Accessed Keys:")
        for key, count in metrics['most_accessed_keys'][:5]:
            print(f"  {key[:50]}... : {count} times")
        print("="*60)
```

**Benefits:**
- ðŸ“Š Understand cache behavior
- ðŸŽ¯ Identify optimization opportunities
- ðŸ’¡ Data-driven tuning

---

## ðŸŽ¯ Implementation Priority

### Quick Wins (Implement First)
1. **Query Normalization** (1 hour)
2. **Cache Monitoring** (2 hours)
3. **Cache Warming** (1 hour)

### High Impact (Implement Next)
4. **Semantic Caching** (4-6 hours)
5. **Multi-Level Caching** (4-6 hours)
6. **Adaptive Sizing** (3-4 hours)

### Advanced (Consider Later)
7. **TTL Support** (2-3 hours)
8. **Compression** (2-3 hours)
9. **Partial Caching** (4-6 hours)
10. **Batch Operations** (2 hours)

---

## ðŸ“ˆ Expected Improvements

| Optimization | Hit Rate Improvement | Cost Savings | Implementation |
|--------------|---------------------|--------------|----------------|
| Semantic Caching | +20-40% | High | Medium |
| Query Normalization | +10-15% | Medium | Easy |
| Cache Warming | +30% (cold start) | High | Easy |
| Multi-Level | +15-25% | High | Hard |
| Adaptive Sizing | +5-10% | Medium | Medium |
| TTL | Varies | Medium | Easy |
| Monitoring | 0% (enables optimization) | - | Easy |

**Combined:** With top 3 optimizations, expect **50-70% hit rate improvement!**

---

## ðŸš€ Next Steps

1. **Start with monitoring** to understand current behavior
2. **Implement query normalization** for quick wins
3. **Add semantic caching** for biggest impact
4. **Test and measure** improvements
5. **Iterate** based on data

Would you like me to implement any of these optimizations for your codebase?

