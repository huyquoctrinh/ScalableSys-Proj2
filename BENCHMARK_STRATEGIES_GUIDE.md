# Cache Strategy Benchmark Guide

## üéØ Overview

This benchmark compares **three caching strategies** with comprehensive token throughput metrics:

1. **No Cache** - Baseline (full pipeline every time)
2. **LRU Cache (Basic)** - Standard caching
3. **LRU Cache + Query Normalizer** - Optimized with normalization

---

## üìä Metrics Tracked

### Token Throughput (NEW!)

#### 1. **Aggregate Token Throughput**
```
Total tokens generated / Total time
= Tokens per second for entire test dataset
```

**Example:** 10,000 tokens in 100 seconds = 100 tokens/sec aggregate

#### 2. **Per-Query Token Throughput**
```
Tokens per query / Latency per query
= Average tokens/sec for individual queries
```

**Example:** Query 1: 50 tokens/sec, Query 2: 75 tokens/sec ‚Üí Avg: 62.5 tokens/sec

#### 3. **Why Both Matter**

| Metric | What It Shows | Use Case |
|--------|--------------|----------|
| **Aggregate** | Overall system throughput | Capacity planning, cost estimation |
| **Per-Query** | Individual query performance | User experience, latency analysis |

---

## üöÄ Running the Benchmark

### Quick Start

```bash
python benchmark_cache_strategies.py
```

### What It Does

1. **Setup** (one-time)
   - Connects to Kuzu database
   - Initializes DSPy components
   - Loads exemplars

2. **Generates Test Queries** (30 queries with variations)
   - Original questions
   - Case variations (uppercase/lowercase)
   - Punctuation variations
   - Tests cache effectiveness

3. **Runs Three Benchmarks**
   - No Cache (baseline)
   - LRU Cache (basic)
   - LRU Cache + Normalizer (optimized)

4. **Generates Reports**
   - Comprehensive comparison table
   - Token throughput analysis
   - Performance improvements
   - Saves results to JSON

---

## üìà Sample Output

### During Execution

```
================================================================================
üîÑ Running benchmark: 2. LRU Cache (Basic)
   Processing 30 queries...
   Normalizer: DISABLED
================================================================================
   Progress: 5/30 | Hits: 2 (40.0%) | Tokens: 612 | Avg tokens/sec: 25.3
   Progress: 10/30 | Hits: 5 (50.0%) | Tokens: 1,234 | Avg tokens/sec: 28.7
   ...
   ‚úÖ Completed 30 queries
   üìä Cache hit rate: 55.0%
   üìù Total tokens: 3,456
   ‚ö° Aggregate throughput: 32.5 tokens/sec
   üìà Avg per-query throughput: 45.2 tokens/sec
```

### Final Comparison Report

```
==================================================================================================
üìä COMPREHENSIVE CACHE STRATEGY COMPARISON
==================================================================================================

TOKEN THROUGHPUT METRICS
==================================================================================================
Strategy                       Aggregate       Avg Per-Query   Median Per-Query
                               (tokens/sec)    (tokens/sec)    (tokens/sec)
--------------------------------------------------------------------------------------------------
1. No Cache                    18.5            22.3            21.5
2. LRU Cache (Basic)           32.5            45.2            42.8
3. LRU Cache + Normalizer      38.7            52.1            49.3

QUERY THROUGHPUT & TOKENS
==================================================================================================
Strategy                       Queries/sec     Total Tokens    Avg Tokens/Query
--------------------------------------------------------------------------------------------------
1. No Cache                    0.125           3,450           115.0
2. LRU Cache (Basic)           0.234           3,465           115.5
3. LRU Cache + Normalizer      0.287           3,458           115.3

CACHE PERFORMANCE
==================================================================================================
Strategy                       Hit Rate        Hits       Misses     Total
--------------------------------------------------------------------------------------------------
1. No Cache                    0.0%            0          30         30
2. LRU Cache (Basic)           53.3%           16         14         30
3. LRU Cache + Normalizer      66.7%           20         10         30

PERFORMANCE IMPROVEMENT vs NO CACHE
==================================================================================================
Strategy                       Latency         Token Throughput    Time Saved
                               Improvement     (Aggregate)         (seconds)
--------------------------------------------------------------------------------------------------
1. No Cache                    baseline        baseline            baseline
2. LRU Cache (Basic)           46.5%           +75.7%              112.3s
3. LRU Cache + Normalizer      56.8%           +109.2%             136.8s

SPEEDUP FACTORS vs NO CACHE
==================================================================================================
Strategy                       Latency Speedup         Throughput Speedup
--------------------------------------------------------------------------------------------------
1. No Cache                    1.00x                   1.00x
2. LRU Cache (Basic)           1.87x                   1.76x
3. LRU Cache + Normalizer      2.31x                   2.09x
```

---

## üîç Understanding the Metrics

### 1. **Aggregate vs Per-Query Throughput**

#### Aggregate Throughput
```python
# Total performance across all queries
aggregate_throughput = total_tokens / total_time

# Example:
# 3,450 tokens in 186 seconds = 18.5 tokens/sec
```

**What it tells you:**
- Overall system capacity
- Total work done per second
- Good for: capacity planning, cost estimation

#### Per-Query Throughput
```python
# Average performance per individual query
per_query_throughput = mean([tokens/latency for each query])

# Example:
# Query 1: 120 tokens / 5.2s = 23.1 tokens/sec
# Query 2: 100 tokens / 0.8s = 125.0 tokens/sec (cache hit!)
# Average: 74.0 tokens/sec
```

**What it tells you:**
- Individual query performance
- User experience (some queries much faster)
- Good for: SLA planning, user expectations

### 2. **Why They Differ**

```
No Cache:
- Aggregate: 18.5 tokens/sec  (consistent)
- Per-Query: 22.3 tokens/sec  (slightly higher due to measurement)

LRU Cache:
- Aggregate: 32.5 tokens/sec  (2x faster)
- Per-Query: 45.2 tokens/sec  (2x faster, includes fast cache hits)

The difference grows with caching because:
- Cache hits are very fast (100+ tokens/sec)
- Cache misses are slow (15-20 tokens/sec)
- Average per-query is pulled up by fast hits
- Aggregate accounts for total time including slow misses
```

---

## üí° Key Insights

### Cache Hit Rate Impact

```
Query Normalizer adds 10-20% hit rate:

Without Normalizer:
"Who won Physics?"       ‚Üí Cache key 1
"who won physics?"       ‚Üí Cache key 2 (miss!)
"WHO WON PHYSICS?"       ‚Üí Cache key 3 (miss!)

With Normalizer:
"Who won Physics?"       ‚Üí "who won physics" ‚Üí Cache key 1
"who won physics?"       ‚Üí "who won physics" ‚Üí Cache key 1 (hit!)
"WHO WON PHYSICS?"       ‚Üí "who won physics" ‚Üí Cache key 1 (hit!)
```

### Throughput Improvements

| Strategy | Aggregate Speedup | Per-Query Speedup | Hit Rate |
|----------|------------------|-------------------|----------|
| No Cache | 1.0x | 1.0x | 0% |
| LRU Basic | 1.8x | 2.0x | 50-55% |
| LRU + Normalizer | 2.1x | 2.3x | 65-70% |

---

## üéØ Customization

### Adjust Sample Size

```python
# In main() function, around line 440:
sample_size = 50  # Increase for more comprehensive test
```

### Change Base Questions

```python
# In main() function, around line 428:
base_questions = [
    "Your question here",
    "Another question",
    # Add more...
]
```

### Modify Cache Size

```python
# In main() function, around line 456:
CacheStrategy("2. LRU Cache (Basic)", 
              LRUDataManager(cache_size=256),  # Larger cache
              use_normalizer=False),
```

---

## üìä Output Files

### JSON Results
```json
{
  "timestamp": "2025-11-27 10:30:00",
  "strategies": [
    {
      "name": "1. No Cache",
      "stats": {
        "total_queries": 30,
        "throughput_tokens_per_sec_aggregate": 18.5,
        "avg_tokens_per_sec_per_query": 22.3,
        ...
      },
      "per_query_metrics": [
        {"latency": 8.2, "tokens": 125, "tokens_per_sec": 15.2},
        ...
      ]
    },
    ...
  ]
}
```

**Saved to:** `cache_strategies_results.json`

---

## üî¨ Analysis Examples

### Example 1: Comparing Strategies

```python
import json

# Load results
with open('cache_strategies_results.json', 'r') as f:
    data = json.load(f)

# Compare aggregate throughput
for strategy in data['strategies']:
    name = strategy['name']
    agg_throughput = strategy['stats']['throughput_tokens_per_sec_aggregate']
    per_query_throughput = strategy['stats']['avg_tokens_per_sec_per_query']
    
    print(f"{name}:")
    print(f"  Aggregate: {agg_throughput:.1f} tokens/sec")
    print(f"  Per-Query: {per_query_throughput:.1f} tokens/sec")
    print(f"  Ratio: {per_query_throughput / agg_throughput:.2f}x")
```

### Example 2: Per-Query Analysis

```python
# Analyze per-query performance
lru_normalized = data['strategies'][2]  # LRU + Normalizer

for i, metric in enumerate(lru_normalized['per_query_metrics'][:5]):
    status = "HIT" if metric['was_hit'] else "MISS"
    print(f"Query {i+1} ({status}): "
          f"{metric['tokens_per_sec']:.1f} tokens/sec "
          f"({metric['tokens']} tokens in {metric['latency']:.2f}s)")
```

Output:
```
Query 1 (MISS): 18.2 tokens/sec (120 tokens in 6.59s)
Query 2 (MISS): 16.8 tokens/sec (110 tokens in 6.55s)
Query 3 (HIT): 125.0 tokens/sec (100 tokens in 0.80s)
Query 4 (MISS): 19.5 tokens/sec (130 tokens in 6.67s)
Query 5 (HIT): 135.7 tokens/sec (95 tokens in 0.70s)
```

---

## üéì Understanding Results

### Good Results

‚úÖ **LRU + Normalizer should show:**
- 10-20% higher hit rate than basic LRU
- 2-3x aggregate throughput vs no cache
- 60-70% overall hit rate
- Per-query throughput 2-3x higher than aggregate

### Investigate If:

‚ö†Ô∏è **Hit rate < 50%**
- Check if questions are too diverse
- Consider larger cache size
- Review normalization effectiveness

‚ö†Ô∏è **No improvement with normalizer**
- Check if test questions lack variations
- Add more case/punctuation variations
- Verify normalizer is working

‚ö†Ô∏è **Per-query ‚âà Aggregate throughput**
- May indicate low hit rate
- Cache not providing benefit
- Review cache configuration

---

## üöÄ Production Usage

### Interpret for Production

```python
# From benchmark results
aggregate_throughput = 38.7  # tokens/sec
total_tokens_per_query = 115  # avg

# Production planning
queries_per_second_capacity = aggregate_throughput / total_tokens_per_query
# = 38.7 / 115 = 0.34 queries/sec

# With safety margin (50%)
production_capacity = queries_per_second_capacity * 0.5
# = 0.17 queries/sec = 10 queries/min
```

### Scale Estimates

| Metric | Value | Calculation |
|--------|-------|-------------|
| Single instance capacity | 10 queries/min | From benchmark |
| 100 users (1 q/min each) | 10 instances | 100 / 10 = 10 |
| 1000 users (1 q/min each) | 100 instances | 1000 / 10 = 100 |

---

## üí° Best Practices

### 1. Run Multiple Times

```bash
# Run 3 times and average
for i in {1..3}; do
    echo "Run $i"
    python benchmark_cache_strategies.py
    sleep 60
done
```

### 2. Test with Real Queries

Replace `base_questions` with actual production queries for accurate results.

### 3. Monitor Cache Size

```python
# After benchmark
print(f"Cache filled: {len(cache.cache)}/{cache.cache.maxsize}")
```

If cache is full, consider increasing size.

### 4. Vary Test Patterns

```python
# More repetitions = higher hit rate
repeat_factor = 5  # More realistic for production

# Fewer repetitions = lower hit rate (more diverse)
repeat_factor = 2  # Stress test
```

---

## ‚úÖ Summary

### What This Benchmark Provides

1. ‚úÖ **Three strategy comparison**
2. ‚úÖ **Aggregate token throughput** (system-wide)
3. ‚úÖ **Per-query token throughput** (individual)
4. ‚úÖ **Cache performance metrics**
5. ‚úÖ **Performance improvements**
6. ‚úÖ **Speedup factors**
7. ‚úÖ **Detailed JSON output**
8. ‚úÖ **Key insights summary**

### Typical Results

- **No Cache**: 15-20 tokens/sec aggregate
- **LRU Basic**: 30-35 tokens/sec (1.8x faster)
- **LRU + Normalizer**: 38-45 tokens/sec (2.1x faster)

### Key Takeaway

**Query Normalizer adds 10-20% hit rate and 15-20% throughput improvement over basic LRU caching with minimal overhead!**

---

*Cache strategy benchmark guide - November 27, 2025*

